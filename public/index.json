
[{"content":"","date":"13 October 2025","externalUrl":null,"permalink":"/","section":"Connor Bryan","summary":"","title":"Connor Bryan","type":"page"},{"content":"","date":"13 October 2025","externalUrl":null,"permalink":"/tags/gravitational_waves/","section":"Tags","summary":"","title":"Gravitational_waves","type":"tags"},{"content":"","date":"13 October 2025","externalUrl":null,"permalink":"/tags/machine_learning/","section":"Tags","summary":"","title":"Machine_learning","type":"tags"},{"content":"During the course of my Master\u0026rsquo;s degree in Data Intensive Physics, I spent my time learning how machine learning algorithms work and how to analyze gravitational wave signals. The main way we analyzed the data from gravitational waves was to do parameter estimation to try and estimate some of the parameters of the black holes that merged to create the waveform. During my \u0026lsquo;Observational Gravitational Wave Astronomy\u0026rsquo; module, this culminated in a final project where we used Markov Chain Monte Carlo methods to estimate the parameters of GW150914, which was the first detected signal. This analysis consisted of using a surrogate generator (a program to generate a waveform), overlapping the generated waveform with the detected signal, measuring how similar they are, and repeating this process until we find the best match between the generated and measured signals. As you could probably tell, one of the bottlenecks in this system is the generation of the waveforms. There are multiple waveform families, which I won\u0026rsquo;t go into in this post, but the main ones we looked at are Inspiral-Merger-Ringdown Phenomenological models (IMRPhenom) and Effective-One-Body models (EOB). These models simulate the same merger but use different mathematics to accomplish that. The IMR waveforms are less accurate but faster to generate, while the EOB models are more accurate but take a lot longer to produce. To overcome the generation time constraint, researchers generate \u0026ldquo;Waveform banks,\u0026rdquo; which are precomputed waveforms that can be used within parameter estimation pipelines. These waveform banks typically contain 100,000 to 1,000,000 signals, making them large and time-consuming to generate.\n! For my summer project, I ended up working on a parameter estimation pipeline using Simulation-Based Inference (SBI). TL;DR: SBI works by running lots of realistic simulations and letting a machine-learning model learn which source parameters match the observed data. This makes it much faster and easier to obtain reliable parameter estimates and uncertainties. To train a reliable model, you require a massive waveform bank, which would take a long time to generate. This is where my summer project came in, as I wanted to create a model that would learn the function of the mergers and generate waveforms quickly.\nBackground # Initially, I thought of using Fourier Neural Operators (FNO) to learn the partial derivative functions behind the black hole evolution equations. FNOs are machine learning networks that learn partial differential equations and have been used to model systems like fluid evolution (https://zongyi-li.github.io/blog/2020/fourier-pde/). The idea was that the model would take in some randomly generated input parameters and produce a gravitational wave signal. It didn\u0026rsquo;t work. The problem with trying to learn an oscillating signal is spectral bias, where a model learns the low-frequency target first and fails at learning the high-frequency part. This results in a signal that generalises well initially but then fails spectacularly.\n! To solve this I needed to either:\nUse a model which could work around spectral bias. Or simplify the input we are trying to learn. I went with both options. The Model # To work around spectral bias, I split my model into smaller sub-models, which we will call banks, and had them learn a small section of the target waveform. This way, the model doesn\u0026rsquo;t just learn the low-frequency and high-frequency sections at the same time, but the final signal is the summation of all the predictions from the smaller models. This parallelisation makes learning easier.\nThe next way to make it easier to learn, is to change the input data. Oscillating data is hard to model so I though about decomposing it into an Amplitude and Phase target. By learning and predicting amplitude and phase, the waveform can be reconstructed by the formula, $$h_+(t) = A(t)cos(\\phi(t)),h_\\times = A(t)sin(\\phi(t))$$ Where $h_+$ and $h_\\times$ represent the polarisation of the signal. In the plots below I have the amplitude and phase change over time. The amplitude is the strength of the signal, and the phase follows where the objects are in their orbit.\n! These are much simpler and easier to work with, and as a result, we get a model that can learn the functions well. We now have the inputs and targets for our model. We use the masses and spins of the black holes as inputs and have the amplitude and phase as targets. I generated the data using PyCBC and decomposed the generated waveform into the two arrays ready for training. The figure below shows the network structure. I joined the normalised arrays for both targets with the input parameters and trained a regression network. There are two networks that need to be trained, one for each target. These networks are made up of $k$ \u0026ldquo;banks,\u0026rdquo; which are subnetworks used to make the learning process easier. After they are trained, we can use the amplitude and phase predictions to reconstruct the model.\n! A few additional things the network does involves estimating the uncertainty of its predictions using a Laplacian (Laplace) approximation over the final weights of the amplitude network.\nImagine the network’s loss landscape as a mountain range spread over a grid where each point in that grid represents a possible configuration of the network’s weights. The valleys in this range correspond to regions where the model fits the data well. Now, the Laplacian doesn’t measure how high the mountains are, instead, it measures how sharp or wide the valleys are around their lowest points. A steep, narrow valley means the network is confident as small shifts in weights quickly make things worse, while a wide, gentle valley means it’s less certain. In essence, the Laplacian captures the curvature of that landscape, turning the hidden shape of the mountains into a measure of how confident the model is in its predictions. After completing this network, I tested it against PyCBC for generation speed and accuracy.\nResults # Speed of Inference # The first test I did was generate a bunch of waveforms using PyCBC and my network to compare how fast the network is at inference. This network uses GPU batch generation, where I can get an array of inputs and, in one forward pass, obtain the corresponding outputs. The table below shows this comparison for generating 10, 100, and 1000 waveforms in seconds. For single inference, my model was comparable to the IMRPhenomD approximant. However, for batch generation, it was significantly faster.\nModel $n=10$ $n=100$ $n=1000$ SEOBNRv4 1.954 19.601 196.136 IMRPhenomD 0.102 0.557 5.627 My Model Single Gen 0.318 0.548 5.940 My Model Batch Gen 0.013 0.138 1.406 This is great and all, but if the model isn\u0026rsquo;t accurate, this research has been worthless. To test the accuracy, I generated two waveforms using the same prior parameters and used the inner dot product match to see how similar they are. The following was using the IMRPhenomD approximant, and my model achieved a match of $\\approx 0.992$.\n! Accuracy compared to the ground truth # To test the reliability of the model, I conducted the same test with 1000 different prior parameters. The following shows the results. The IMRPhenom model had a mean accuracy of approximately 0.98, whereas the SEOBNRv4 model had an accuracy of approximately 0.610. The low accuracy of the EOB model was due to a poor set of hyperparameters, which I didn\u0026rsquo;t have time to change for my final tests, as the deadline was a day away when the final tests were performed. As a side note, training on a dataset of 10,000 samples took about a day on my PC at home\u0026hellip; To run 50 hyperparameter tuning runs, it took a day per model. My computer isn\u0026rsquo;t even that slow, it has a Radeon RX 6800 graphics card, a Ryzen 5 3600 CPU, and 16 GB of RAM.\n! SBI # Finally, I tested this network in SBI, which is what this entire project was about, a week before the deadline. This definitely didn\u0026rsquo;t do me any favors, as I worked long nights to get this model integrated into a separate codebase. Eventually, however, I managed to get a training run in using my generated waveforms and obtained the following results!\n! Ideally, you want the blue cross to be in the jelly bean shape in the bottom left plot, as this is the injected value. The orange plot is using Bilby, and the green plot is using the SBI project I was a part of. The jelly bean shape on the SBI plot is much larger and off-center compared to Bilby, which could be due to not enough training data or poor hyperparameters in the model. Even though I know it\u0026rsquo;s not better than the pre-established Bilby project, I\u0026rsquo;m just glad the SBI network was able to learn from my generator.\nFinal Thoughts # I set out to create a waveform generator to accelerate the construction of gravitational wave templates for parameter estimation, and I was somewhat successful! My model was able to generate data that matched the input data with a high degree of accuracy, and I was able to integrate the project within another project.\nThis project has a lot of room for expansion:\nTrain more networks on different waveform approximants Train on wider priors, so we can generate a larger waveform bank Update the model to train on noise or even long lived Neutron star mergers This project is still under development and has a GitHub repo at https://github.com/Zeos-ctrl/SPECTRE if you want to play around with it!\n","date":"13 October 2025","externalUrl":null,"permalink":"/posts/making-a-gravitational-waveform-generator-using-machine-learning/","section":"Posts","summary":"\u003cp\u003eDuring the course of my Master\u0026rsquo;s degree in Data Intensive Physics, I spent my time learning how machine learning algorithms work and how to analyze gravitational wave signals. The main way we analyzed the data from gravitational waves was to do parameter estimation to try and estimate some of the parameters of the black holes that merged to create the waveform. During my \u0026lsquo;Observational Gravitational Wave Astronomy\u0026rsquo; module, this culminated in a final project where we used Markov Chain Monte Carlo methods to estimate the parameters of GW150914, which was the first detected signal. This analysis consisted of using a surrogate generator (a program to generate a waveform), overlapping the generated waveform with the detected signal, measuring how similar they are, and repeating this process until we find the best match between the generated and measured signals. As you could probably tell, one of the bottlenecks in this system is the generation of the waveforms. There are multiple waveform families, which I won\u0026rsquo;t go into in this post, but the main ones we looked at are Inspiral-Merger-Ringdown Phenomenological models (IMRPhenom) and Effective-One-Body models (EOB). These models simulate the same merger but use different mathematics to accomplish that. The IMR waveforms are less accurate but faster to generate, while the EOB models are more accurate but take a lot longer to produce. To overcome the generation time constraint, researchers generate \u0026ldquo;Waveform banks,\u0026rdquo; which are precomputed waveforms that can be used within parameter estimation pipelines. These waveform banks typically contain 100,000 to 1,000,000 signals, making them large and time-consuming to generate.\u003c/p\u003e","title":"Making A Gravitational Waveform Generator Using Machine Learning","type":"posts"},{"content":"","date":"13 October 2025","externalUrl":null,"permalink":"/tags/parameter_estimation/","section":"Tags","summary":"","title":"Parameter_estimation","type":"tags"},{"content":"","date":"13 October 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"13 October 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"BSc in Computer Security and MSc in Data-Intensive Physics, specializing in Machine Learning/AI, Gravitational Wave Analysis, and Cryptography. Former GB athlete, I focused on time management and incremental goals to excel in sports. Transitioned to coaching to mentor athletes and foster their growth. Mental health advocate for athletes, founding a movement to promote enjoyment and recognize their value beyond sport. Pursuing a PhD in gravitational wave science or AI ethics\n","date":"7 October 2025","externalUrl":null,"permalink":"/about/","section":"Connor Bryan","summary":"\u003cp\u003eBSc in Computer Security and MSc in Data-Intensive Physics, specializing in Machine Learning/AI, Gravitational Wave Analysis, and Cryptography. Former GB athlete, I focused on time management and incremental goals to excel in sports. Transitioned to coaching to mentor athletes and foster their growth. Mental health advocate for athletes, founding a movement to promote enjoyment and recognize their value beyond sport. Pursuing a PhD in gravitational wave science or AI ethics\u003c/p\u003e","title":"About","type":"page"},{"content":" Experience Link Role Dates Location Tribela Head of Engineering 2025 - Present Hybrid - Oxford, UK Zeos Systems Founder 2024 - 2025 Remote Abingdon Vale Swimming Club Level 1 Swimming Coach 2022 - Present Remote Darren Bryan Security Services Access Control Engineer 2018 - Present Oxford, UK Education Link Degree Date Cardiff University MSc Data Intensive Physics Sept. 2024 - Sept. 2025 (expected) Cardiff Metropolitan BSc Cyber Security (1st Class Hons) Sept. 2021 - Jun. 2024 Oxford City College Extended Diploma in Engineering (MMM) Sep. 2018 - Jun. 2020 ","date":"7 October 2025","externalUrl":null,"permalink":"/resume/","section":"Connor Bryan","summary":"\u003ch2 class=\"relative group\"\u003eExperience \n    \u003cdiv id=\"experience\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth\u003eLink\u003c/th\u003e\n      \u003cth\u003eRole\u003c/th\u003e\n      \u003cth\u003eDates\u003c/th\u003e\n      \u003cth\u003eLocation\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ca href=\"https://tribela.com\" target=\"_blank\"\u003eTribela\u003c/a\u003e\u003c/td\u003e\n      \u003ctd\u003eHead of Engineering\u003c/td\u003e\n      \u003ctd\u003e2025 - Present\u003c/td\u003e\n      \u003ctd\u003eHybrid - Oxford, UK\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ca href=\"http://www.zeos.systems\" target=\"_blank\"\u003eZeos Systems\u003c/a\u003e\u003c/td\u003e\n      \u003ctd\u003eFounder\u003c/td\u003e\n      \u003ctd\u003e2024 - 2025\u003c/td\u003e\n      \u003ctd\u003eRemote\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ca href=\"#\" target=\"_blank\"\u003eAbingdon Vale Swimming Club\u003c/a\u003e\u003c/td\u003e\n      \u003ctd\u003eLevel 1 Swimming Coach\u003c/td\u003e\n      \u003ctd\u003e2022 - Present\u003c/td\u003e\n      \u003ctd\u003eRemote\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ca href=\"https://darrenbryansecurityservices.co.uk\" target=\"_blank\"\u003eDarren Bryan Security Services\u003c/a\u003e\u003c/td\u003e\n      \u003ctd\u003eAccess Control Engineer\u003c/td\u003e\n      \u003ctd\u003e2018 - Present\u003c/td\u003e\n      \u003ctd\u003eOxford, UK\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003ch2 class=\"relative group\"\u003eEducation \n    \u003cdiv id=\"education\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth\u003eLink\u003c/th\u003e\n      \u003cth\u003eDegree\u003c/th\u003e\n      \u003cth\u003eDate\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ca href=\"https://www.cardiff.ac.uk\" target=\"_blank\"\u003eCardiff University\u003c/a\u003e\u003c/td\u003e\n      \u003ctd\u003eMSc Data Intensive Physics\u003c/td\u003e\n      \u003ctd\u003eSept. 2024 - Sept. 2025 (expected)\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ca href=\"https://www.cardiffmet.ac.uk\" target=\"_blank\"\u003eCardiff Metropolitan\u003c/a\u003e\u003c/td\u003e\n      \u003ctd\u003eBSc Cyber Security (1st Class Hons)\u003c/td\u003e\n      \u003ctd\u003eSept. 2021 - Jun. 2024\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ca href=\"#\" target=\"_blank\"\u003eOxford City College\u003c/a\u003e\u003c/td\u003e\n      \u003ctd\u003eExtended Diploma in Engineering (MMM)\u003c/td\u003e\n      \u003ctd\u003eSep. 2018 - Jun. 2020\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"Resume","type":"page"},{"content":"","date":"6 October 2025","externalUrl":null,"permalink":"/tags/github/","section":"Tags","summary":"","title":"Github","type":"tags"},{"content":"","date":"6 October 2025","externalUrl":null,"permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":"For a while, I\u0026rsquo;ve wanted to start a blog to share my ideas and try to articulate what I\u0026rsquo;ve learned. Before this iteration of the blog, I tried using a template YAML file within JavaScript to create blog posts. This approach was terrible; it was hard to see any syntax or even format it properly afterward. Sometimes, it\u0026rsquo;s better to use already developed tools and techniques. That\u0026rsquo;s where I found an amazing video by NetworkChuck about setting up a content pipeline using Obsidian to create the Markdown files for a blog, and using a templater such as Hugo to turn them into pretty webpages like the one you are reading now. In this post, I will go over the steps I took to create this blog, and hopefully inspire you to create one as it\u0026rsquo;s so easy to achieve.\nSetting up Obsidian # The first thing we will do is install Obsidian and set up our blog posts directory. Obsidian is a note-taking and organisation tool that I\u0026rsquo;ve started to love using. You can organise markdown notes and content in folders to create a \u0026ldquo;second brain.\u0026rdquo; There are tools to set up personalised daily notes so you can track habits and create diaries. This content is synced between devices on your account as well. Download the app from https://obsidian.md/ and go through the setup.\nAfter you have setup Obsidian:\nCreate a new folder where you will put all you blog posts, I labelled mine \u0026ldquo;posts\u0026rdquo;. You can start to write your posts in this directory, all you need now is the path to the directory. Right click the folder and select \u0026ldquo;reveal in explorer\u0026rdquo;. Keep a note of this path as we will need it later. In order for the blog to have a title and tags we need to set some properties, all you need to add to the top of your document is.\n--- title: Making a Tech Blog using Hugo and Obsidian date: 2025-10-06 draft: false tags: - tag1 - tag2 --- Setting up Hugo # Firstly, install Git (https://github.com/git-guides/install-git) and Go (https://go.dev/dl/) and follow the steps to install for your system at https://gohugo.io/installation/. The installation is simple for Linux, as you can install it with your package manager. After everything is installed, you can start creating your website.\n## Verify Hugo works hugo version ## Create a new site hugo new site sitename cd sitename Update \u0026ldquo;sitename\u0026rdquo; with the project name. Next, you need to theme your site. My site uses Blowfish as a theme (https://blowfish.page/); however, an entire list of themes can be found here: https://themes.gohugo.io/. Follow the instructions to install the theme in your Hugo site. The easiest way is to use a Git submodule, however. WARNING: These commands will differ depending on the theme; follow the instructions on the theme page.\n## Initialize a git repository (Make sure you are in your Hugo website directory) git init ## Set global username and email parameters for git git config --global user.name \u0026#34;Your Name\u0026#34; git config --global user.email \u0026#34;your.email@example.com\u0026#34; ## Install a theme (we are installing the Blowfish theme here) ## Find a theme ---\u0026gt; https://themes.gohugo.io/ git submodule add -b main https://github.com/nunocoracao/blowfish.git themes/blowfish ## Delete the hugo generated config and copy the blowfish configs into config/_default mkdir config/_default cp themes/blowfish/config/_default/* config/_default/ rm hugo.toml You can go through and update the config so you site looks how you want it to. After you\u0026rsquo;ve configured your site you can now test it with:\n## Verify Hugo works with your theme by running this command hugo server -t themename Getting content on your site # All your content for the website lives in the \u0026ldquo;content\u0026rdquo; folder. If you have different pages, you need a corresponding folder within the content directory. For our blog posts, we will create a directory called \u0026ldquo;posts\u0026rdquo; in the content folder and copy our content from Obsidian into this folder.\nFor Windows use:\nrobocopy sourcepath destination path /mir For Mac/Linux use:\nrsync -av --delete \u0026#34;sourcepath\u0026#34; \u0026#34;destinationpath\u0026#34; Finally, to add images to these posts, we will use a script made by NetworkCheck. He explains it in his video, which I highly recommend watching as a follow-along. Make sure you have the correct paths to your own Obsidian Vault and Hugo site. These are Python scripts, so make sure you have Python installed to run them.\nFor Windows:\nimport os import re import shutil # Paths (using raw strings to handle Windows backslashes correctly) posts_dir = r\u0026#34;C:\\Users\\chuck\\Documents\\chuckblog\\content\\posts\u0026#34; attachments_dir = r\u0026#34;C:\\Users\\chuck\\Documents\\my_second_brain\\neotokos\\Attachments\u0026#34; static_images_dir = r\u0026#34;C:\\Users\\chuck\\Documents\\chuckblog\\static\\images\u0026#34; # Step 1: Process each markdown file in the posts directory for filename in os.listdir(posts_dir): if filename.endswith(\u0026#34;.md\u0026#34;): filepath = os.path.join(posts_dir, filename) with open(filepath, \u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as file: content = file.read() # Step 2: Find all image links in the format ![Image Description](/images/Pasted%20image%20...%20.png) images = re.findall(r\u0026#39;\\[\\[([^]]*\\.png)\\]\\]\u0026#39;, content) # Step 3: Replace image links and ensure URLs are correctly formatted for image in images: # Prepare the Markdown-compatible link with %20 replacing spaces markdown_image = f\u0026#34;![Image Description](/images/{image.replace(\u0026#39; \u0026#39;, \u0026#39;%20\u0026#39;)})\u0026#34; content = content.replace(f\u0026#34;[[{image}]]\u0026#34;, markdown_image) # Step 4: Copy the image to the Hugo static/images directory if it exists image_source = os.path.join(attachments_dir, image) if os.path.exists(image_source): shutil.copy(image_source, static_images_dir) # Step 5: Write the updated content back to the markdown file with open(filepath, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as file: file.write(content) print(\u0026#34;Markdown files processed and images copied successfully.\u0026#34;) And for Mac/Linux\nimport os import re import shutil # Paths posts_dir = \u0026#34;/Users/networkchuck/Documents/chuckblog/content/posts/\u0026#34; attachments_dir = \u0026#34;/Users/networkchuck/Documents/neotokos/Attachments/\u0026#34; static_images_dir = \u0026#34;/Users/networkchuck/Documents/chuckblog/static/images/\u0026#34; # Step 1: Process each markdown file in the posts directory for filename in os.listdir(posts_dir): if filename.endswith(\u0026#34;.md\u0026#34;): filepath = os.path.join(posts_dir, filename) with open(filepath, \u0026#34;r\u0026#34;) as file: content = file.read() # Step 2: Find all image links in the format ![Image Description](/images/Pasted%20image%20...%20.png) images = re.findall(r\u0026#39;\\[\\[([^]]*\\.png)\\]\\]\u0026#39;, content) # Step 3: Replace image links and ensure URLs are correctly formatted for image in images: # Prepare the Markdown-compatible link with %20 replacing spaces markdown_image = f\u0026#34;![Image Description](/images/{image.replace(\u0026#39; \u0026#39;, \u0026#39;%20\u0026#39;)})\u0026#34; content = content.replace(f\u0026#34;[[{image}]]\u0026#34;, markdown_image) # Step 4: Copy the image to the Hugo static/images directory if it exists image_source = os.path.join(attachments_dir, image) if os.path.exists(image_source): shutil.copy(image_source, static_images_dir) # Step 5: Write the updated content back to the markdown file with open(filepath, \u0026#34;w\u0026#34;) as file: file.write(content) print(\u0026#34;Markdown files processed and images copied successfully.\u0026#34;) Congratulations! After all these steps are done, we can run the Hugo server locally and hopefully see our formatted blog posts. This is all well and good; however, if we want anyone to be able to read these, we need to get them hosted on the internet. Luckily, GitHub has a free hosting service called \u0026ldquo;GitHub Pages.\nSetting up GitHub Pages # GitHub Pages is simple to set up. I will be following the official instructions to walk you through this (https://docs.github.com/en/pages/quickstart). First, make sure you have a GitHub account, create a repository called \u0026ldquo;your username\u0026rdquo;.github.io, and initialize the repository with a README. Next, go into the settings and navigate to the \u0026ldquo;Code and automation\u0026rdquo; section in the sidebar, then select the Pages section. From there, select Deploy from a branch under Source and choose your main branch under the branch setting. Now, any time we commit to this branch, it will update our site. Type your repository\u0026rsquo;s name in the URL bar to see if GitHub Pages has been set up correctly.\nHugo needs specific changes to work with GitHub pages (https://gohugo.io/host-and-deploy/host-on-github-pages/). In the site configuration, add the following to the end (for Blowfish its in config/_default/config.toml):\n[caches] [caches.images] dir = \u0026#39;:cacheDir/images\u0026#39; Now create a GitHub workflows directory and a configuration for deploying Hugo.\nmkdir -p .github/workflows touch .github/workflows/hugo.yaml Copy the following config into the hugo.yaml file we just created.\nname: Build and deploy on: push: branches: - main workflow_dispatch: permissions: contents: read pages: write id-token: write concurrency: group: pages cancel-in-progress: false defaults: run: shell: bash jobs: build: runs-on: ubuntu-latest env: DART_SASS_VERSION: 1.93.2 GO_VERSION: 1.25.1 HUGO_VERSION: 0.151.0 NODE_VERSION: 22.18.0 TZ: Europe/Oslo steps: - name: Checkout uses: actions/checkout@v5 with: submodules: recursive fetch-depth: 0 - name: Setup Go uses: actions/setup-go@v5 with: go-version: ${{ env.GO_VERSION }} cache: false - name: Setup Node.js uses: actions/setup-node@v4 with: node-version: ${{ env.NODE_VERSION }} - name: Setup Pages id: pages uses: actions/configure-pages@v5 - name: Create directory for user-specific executable files run: | mkdir -p \u0026#34;${HOME}/.local\u0026#34; - name: Install Dart Sass run: | curl -sLJO \u0026#34;https://github.com/sass/dart-sass/releases/download/${DART_SASS_VERSION}/dart-sass-${DART_SASS_VERSION}-linux-x64.tar.gz\u0026#34; tar -C \u0026#34;${HOME}/.local\u0026#34; -xf \u0026#34;dart-sass-${DART_SASS_VERSION}-linux-x64.tar.gz\u0026#34; rm \u0026#34;dart-sass-${DART_SASS_VERSION}-linux-x64.tar.gz\u0026#34; echo \u0026#34;${HOME}/.local/dart-sass\u0026#34; \u0026gt;\u0026gt; \u0026#34;${GITHUB_PATH}\u0026#34; - name: Install Hugo run: | curl -sLJO \u0026#34;https://github.com/gohugoio/hugo/releases/download/v${HUGO_VERSION}/hugo_extended_${HUGO_VERSION}_linux-amd64.tar.gz\u0026#34; mkdir \u0026#34;${HOME}/.local/hugo\u0026#34; tar -C \u0026#34;${HOME}/.local/hugo\u0026#34; -xf \u0026#34;hugo_extended_${HUGO_VERSION}_linux-amd64.tar.gz\u0026#34; rm \u0026#34;hugo_extended_${HUGO_VERSION}_linux-amd64.tar.gz\u0026#34; echo \u0026#34;${HOME}/.local/hugo\u0026#34; \u0026gt;\u0026gt; \u0026#34;${GITHUB_PATH}\u0026#34; - name: Verify installations run: | echo \u0026#34;Dart Sass: $(sass --version)\u0026#34; echo \u0026#34;Go: $(go version)\u0026#34; echo \u0026#34;Hugo: $(hugo version)\u0026#34; echo \u0026#34;Node.js: $(node --version)\u0026#34; - name: Install Node.js dependencies run: | [[ -f package-lock.json || -f npm-shrinkwrap.json ]] \u0026amp;\u0026amp; npm ci || true - name: Configure Git run: | git config core.quotepath false - name: Cache restore id: cache-restore uses: actions/cache/restore@v4 with: path: ${{ runner.temp }}/hugo_cache key: hugo-${{ github.run_id }} restore-keys: hugo- - name: Build the site run: | hugo \\ --gc \\ --minify \\ --baseURL \u0026#34;${{ steps.pages.outputs.base_url }}/\u0026#34; \\ --cacheDir \u0026#34;${{ runner.temp }}/hugo_cache\u0026#34; - name: Cache save id: cache-save uses: actions/cache/save@v4 with: path: ${{ runner.temp }}/hugo_cache key: ${{ steps.cache-restore.outputs.cache-primary-key }} - name: Upload artifact uses: actions/upload-pages-artifact@v3 with: path: ./public deploy: environment: name: github-pages url: ${{ steps.deployment.outputs.page_url }} runs-on: ubuntu-latest needs: build steps: - name: Deploy to GitHub Pages id: deployment uses: actions/deploy-pages@v4 Now we can commit all our changes to the repo and the site should automatically be generated. Remember to set our repository as the remote on the Hugo project.\nResults # After you commit to the repo, you should have a Hugo site that can be updated from our Obsidian Vault. This has been my first blog post written using this pipeline, and it makes a massive difference. Not only has this been fun to write, but it\u0026rsquo;s also been incredibly easy. Automatically seeing the formatted changes gives me more freedom in some design decisions and makes storing these blog posts much easier.\nI hope you\u0026rsquo;ve enjoyed setting this up and learnt a lot in the process :)\n","date":"6 October 2025","externalUrl":null,"permalink":"/posts/making-a-tech-blog-using-hugo-and-obsidian/","section":"Posts","summary":"\u003cp\u003eFor a while, I\u0026rsquo;ve wanted to start a blog to share my ideas and try to articulate what I\u0026rsquo;ve learned. Before this iteration of the blog, I tried using a template YAML file within JavaScript to create blog posts. This approach was terrible; it was hard to see any syntax or even format it properly afterward. Sometimes, it\u0026rsquo;s better to use already developed tools and techniques. That\u0026rsquo;s where I found an amazing video by NetworkChuck about setting up a content pipeline using Obsidian to create the Markdown files for a blog, and using a templater such as Hugo to turn them into pretty webpages like the one you are reading now. In this post, I will go over the steps I took to create this blog, and hopefully inspire you to create one as it\u0026rsquo;s so easy to achieve.\u003c/p\u003e","title":"Making a Tech Blog using Hugo and Obsidian","type":"posts"},{"content":"","date":"6 October 2025","externalUrl":null,"permalink":"/tags/obsidian/","section":"Tags","summary":"","title":"Obsidian","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]